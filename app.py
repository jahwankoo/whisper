import os
import io
import queue
import threading
import tempfile
from datetime import datetime, timedelta

import av
import numpy as np
import streamlit as st
from scipy.io.wavfile import write
from streamlit_webrtc import WebRtcMode, webrtc_streamer, RTCConfiguration
import whisper

st.set_page_config(page_title="Whisper Streaming STT", layout="centered")
st.title("üé§ Whisper Streaming STT (Browser Mic ‚Üí Live Captions)")

# ---------- Whisper Î™®Îç∏ Î°úÎî© (Ï∫êÏãú) ----------
@st.cache_resource
def load_model(model_name="base"):
    # base/medium/large-v3 Îì± ÏÑ†ÌÉù Í∞ÄÎä•
    return whisper.load_model(model_name)

model_name = st.selectbox("Model", ["base", "small", "medium", "large-v3"], index=0)
model = load_model(model_name)

# ---------- ÌååÎùºÎØ∏ÌÑ∞ ----------
SAMPLE_RATE = 48000        # WebRTC Í∏∞Î≥∏
CHANNELS = 1               # Îã®Ïùº Ï±ÑÎÑêÎ°ú Îã§Ïö¥ÎØπÏä§
CHUNK_SECONDS = st.slider("Chunk seconds", 2, 10, 5)   # Î™á Ï¥à Îã®ÏúÑÎ°ú ÏûòÎùºÏÑú Ï†ÑÏÇ¨Ìï†ÏßÄ
OVERLAP_SECONDS = st.slider("Overlap seconds", 0, 3, 1)  # Í≤ΩÍ≥Ñ Îã®Ïñ¥ Î≥¥ÏôÑÏö©
LANG = st.selectbox("Language (optional)", ["auto", "ko", "en"], index=0)
AUTO_LANG = (LANG == "auto")

# ---------- ÏÉÅÌÉú ----------
if "audio_q" not in st.session_state:
    st.session_state.audio_q = queue.Queue()
if "caption" not in st.session_state:
    st.session_state.caption = ""
if "running" not in st.session_state:
    st.session_state.running = False
if "last_saved_wav" not in st.session_state:
    st.session_state.last_saved_wav = None

caption_box = st.empty()
status_box = st.empty()

# ---------- Ïò§ÎîîÏò§ ÏàòÏã† Ï≤òÎ¶¨ ----------
class AudioProcessor:
    def __init__(self):
        self.buffer = []

    def recv(self, frame: av.AudioFrame):
        # float32 ndarray [channels, samples]
        pcm = frame.to_ndarray().astype(np.float32)
        # Îã§Ïö¥ÎØπÏä§ ‚Üí mono
        if pcm.ndim == 2 and pcm.shape[0] > 1:
            pcm = np.mean(pcm, axis=0, keepdims=True)
        st.session_state.audio_q.put(pcm.squeeze())

# ---------- Ï†ÑÏÇ¨Ïö© ÏõåÏª§ Ïä§Î†àÎìú ----------
def transcribe_worker():
    """
    Ïò§ÎîîÏò§ ÌÅêÏóêÏÑú ÏÉòÌîåÏùÑ ÏùΩÏñ¥ ÏùºÏ†ï Í∏∏Ïù¥(CHUNK_SECONDS)ÎßàÎã§ WAVÎ°ú Ï†ÄÏû• ÌõÑ Whisper Ï†ÑÏÇ¨,
    Í≤∞Í≥ºÎ•º ÏûêÎßâÏ∞ΩÏóê ÎàÑÏ†Å ÌëúÏãú. Í≤ΩÍ≥Ñ ÌíàÏßàÏùÑ ÏúÑÌï¥ OVERLAP_SECONDS ÎßåÌÅº ÏïûÎ∂ÄÎ∂ÑÏùÑ Ìï©Ïπ®.
    """
    sr = SAMPLE_RATE
    chunk_len = int(CHUNK_SECONDS * sr)
    overlap_len = int(OVERLAP_SECONDS * sr)

    ring = np.zeros(0, dtype=np.float32)
    last_tail = np.zeros(0, dtype=np.float32)

    status_box.info("Listening‚Ä¶ streaming transcription in progress.")

    while st.session_state.running:
        # ÌÅêÏóêÏÑú Í∞ÄÏö© ÏÉòÌîå ÏµúÎåÄÌïú Î™®ÏúºÍ∏∞ (non-block)
        got_any = False
        while True:
            try:
                part = st.session_state.audio_q.get(timeout=0.1)
                ring = np.concatenate([ring, part])
                got_any = True
            except queue.Empty:
                break

        if not got_any:
            continue

        # Ï∂©Î∂ÑÌûà Î™®Ïù¥Î©¥ Ï†ÑÏÇ¨ Ïã§Ìñâ
        while len(ring) >= chunk_len:
            # overlapÏùÑ ÏïûÏóê Î∂ôÏó¨ÏÑú ÏûêÏó∞Ïä§ÎüΩÍ≤å
            start_idx = 0
            seg = ring[start_idx:start_idx+chunk_len]

            # Îã§Ïùå ÎùºÏö¥ÎìúÎ•º ÏúÑÌï¥ ring Ï§ÑÏù¥Í∏∞
            ring = ring[chunk_len:]

            # Í≤ΩÍ≥Ñ Î≥¥ÏôÑ: Ïù¥Ï†Ñ tail + ÌòÑÏû¨ chunk Í≤∞Ìï©
            if overlap_len > 0:
                seg_for_stt = np.concatenate([last_tail, seg])
                last_tail = seg[-overlap_len:].copy()
            else:
                seg_for_stt = seg

            # WAVÎ°ú Ï†ÄÏû• ÌõÑ Ï†ÑÏÇ¨
            with tempfile.TemporaryDirectory() as td:
                wav_path = os.path.join(td, "seg.wav")
                write(wav_path, sr, seg_for_stt)
                # Ïñ∏Ïñ¥ ÏòµÏÖò
                kwargs = {}
                if not AUTO_LANG:
                    kwargs["language"] = LANG
                # Ï†ÑÏÇ¨
                try:
                    result = model.transcribe(wav_path, **kwargs)
                    text = result.get("text", "").strip()
                except Exception as e:
                    text = f"[ERR:{e}]"

            if text:
                # ÏûêÎßâ ÎàÑÏ†Å(ÏµúÍ∑º 10Ï§ÑÎßå Ïú†ÏßÄ)
                new_caption = (st.session_state.caption + " " + text).strip()
                lines = new_caption.split()
                if len(lines) > 500:  # ÎÑàÎ¨¥ Í∏∏Ïñ¥ÏßÄÎ©¥ ÏïûÎ∂ÄÎ∂Ñ Ï†àÎã®
                    new_caption = " ".join(lines[-500:])
                st.session_state.caption = new_caption
                caption_box.markdown(f"**Live captions:**\n\n{st.session_state.caption}")

# ---------- WebRTC ----------
rtc_config = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)
webrtc_ctx = webrtc_streamer(
    key="whisper-streaming",
    mode=WebRtcMode.RECVONLY,
    rtc_configuration=rtc_config,
    media_stream_constraints={"audio": True, "video": False},
)

# ---------- Ïª®Ìä∏Î°§ ----------
col1, col2, col3 = st.columns(3)
with col1:
    if st.button("Start"):
        if not st.session_state.running:
            st.session_state.caption = ""
            st.session_state.running = True
            # ÏõåÏª§ Ïä§Î†àÎìú Í∏∞Îèô
            t = threading.Thread(target=transcribe_worker, daemon=True)
            t.start()
with col2:
    if st.button("Stop"):
        st.session_state.running = False
        status_box.info("Stopped.")
with col3:
    if st.button("Clear Captions"):
        st.session_state.caption = ""
        caption_box.empty()

# ---------- Ïò§ÎîîÏò§ ÌîÑÎ†àÏûÑ ÏàòÏã† Î£®ÌîÑ ----------
if webrtc_ctx and webrtc_ctx.state.playing:
    # WebRTCÏóêÏÑú Ïò§Îäî Ïò§ÎîîÏò§ ÌîÑÎ†àÏûÑÏùÑ ÏßÄÏÜç ÏàòÏã†
    try:
        while True:
            frame = webrtc_ctx.audio_receiver.get_frame(timeout=0.01)
            AudioProcessor().recv(frame)
    except queue.Empty:
        pass
    except Exception:
        pass

# ---------- ÏµúÏ¢Ö ÎÖπÏùå ÌååÏùº Ï†ÄÏû•(ÏÑ†ÌÉù) ----------
st.markdown("---")
st.subheader("Save last N seconds as WAV")
save_sec = st.slider("Seconds to save", 3, 60, 10)
if st.button("Save Snippet"):
    # ÌÅêÏóê ÎÇ®ÏïÑÏûàÎäî Í≤ÉÎì§ÏùÑ Í∞ÄÎä•Ìïú ÎßåÌÅº Î™®ÏïÑ WAVÎ°ú Ï†ÄÏû•
    samples = []
    try:
        while True:
            samples.append(st.session_state.audio_q.get_nowait())
    except queue.Empty:
        pass
    if samples:
        buf = np.concatenate(samples).astype(np.float32)
        target_len = int(save_sec * SAMPLE_RATE)
        buf = buf[-target_len:] if len(buf) > target_len else buf
        td = tempfile.mkdtemp()
        path = os.path.join(td, f"snippet_{datetime.now().strftime('%H%M%S')}.wav")
        write(path, SAMPLE_RATE, buf)
        st.session_state.last_saved_wav = path
        st.success(f"Saved: {path}")
        st.audio(path)
        with open(path, "rb") as f:
            st.download_button("‚¨áÔ∏è Download WAV", f, file_name=os.path.basename(path))
    else:
        st.warning("No audio buffered yet.")
