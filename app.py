import os
import io
import queue
import threading
import tempfile
from datetime import datetime
import time

import av
import numpy as np
import streamlit as st
from scipy.io.wavfile import write
from streamlit_webrtc import WebRtcMode, webrtc_streamer, RTCConfiguration
import whisper

st.set_page_config(page_title="Whisper Streaming STT", layout="centered")
st.title("ğŸ¤ Whisper Streaming STT (Browser Mic â†’ Live Captions)")

# ---------- Whisper ëª¨ë¸ ë¡œë”© (ìºì‹œ) ----------
@st.cache_resource
def load_model(model_name="base"):
    return whisper.load_model(model_name)

# ë©”ëª¨ë¦¬ ë¬¸ì œë¡œ baseë§Œ ê¶Œì¥
model_name = st.selectbox("Model", ["tiny", "base"], index=1, 
                          help="Streamlit Cloudì—ì„œëŠ” tiny/baseë§Œ ê¶Œì¥")
model = load_model(model_name)

# ---------- íŒŒë¼ë¯¸í„° ----------
SAMPLE_RATE = 16000        # WhisperëŠ” 16kHz ì‚¬ìš©
CHANNELS = 1
CHUNK_SECONDS = st.slider("Chunk seconds", 2, 10, 5)
OVERLAP_SECONDS = st.slider("Overlap seconds", 0, 3, 1)
LANG = st.selectbox("Language", ["auto", "ko", "en"], index=1)

# ---------- ìƒíƒœ ì´ˆê¸°í™” ----------
if "audio_q" not in st.session_state:
    st.session_state.audio_q = queue.Queue()
if "transcript_q" not in st.session_state:
    st.session_state.transcript_q = queue.Queue()
if "caption" not in st.session_state:
    st.session_state.caption = ""
if "running" not in st.session_state:
    st.session_state.running = False
if "worker_thread" not in st.session_state:
    st.session_state.worker_thread = None

# ---------- ì˜¤ë””ì˜¤ í”„ë ˆì„ ì½œë°± ----------
def audio_frame_callback(frame: av.AudioFrame):
    sound = frame.to_ndarray()
    
    # ë¦¬ìƒ˜í”Œë§ (48000 â†’ 16000)
    if frame.sample_rate != SAMPLE_RATE:
        # ê°„ë‹¨í•œ decimation (ì‹¤ì œë¡œëŠ” librosa ì‚¬ìš© ê¶Œì¥)
        ratio = frame.sample_rate // SAMPLE_RATE
        sound = sound[::ratio]
    
    # ëª¨ë…¸ë¡œ ë³€í™˜
    if sound.ndim == 2:
        sound = sound.mean(axis=1)
    
    sound = sound.astype(np.float32)
    st.session_state.audio_q.put(sound)
    
    return frame

# ---------- ì „ì‚¬ ì›Œì»¤ ìŠ¤ë ˆë“œ ----------
def transcribe_worker():
    sr = SAMPLE_RATE
    chunk_len = int(CHUNK_SECONDS * sr)
    overlap_len = int(OVERLAP_SECONDS * sr)
    
    ring = np.zeros(0, dtype=np.float32)
    last_tail = np.zeros(0, dtype=np.float32)
    
    while st.session_state.running:
        # íì—ì„œ ì˜¤ë””ì˜¤ ìˆ˜ì§‘
        collected = []
        try:
            while len(collected) < 10:  # ìµœëŒ€ 10ê°œ ë°°ì¹˜
                part = st.session_state.audio_q.get(timeout=0.1)
                collected.append(part)
        except queue.Empty:
            if not collected:
                continue
        
        if collected:
            ring = np.concatenate([ring] + collected)
        
        # ì¶©ë¶„íˆ ëª¨ì´ë©´ ì „ì‚¬
        while len(ring) >= chunk_len:
            seg = ring[:chunk_len]
            ring = ring[chunk_len:]
            
            # Overlap ì²˜ë¦¬
            if overlap_len > 0 and len(last_tail) > 0:
                seg_for_stt = np.concatenate([last_tail, seg])
                last_tail = seg[-overlap_len:].copy()
            else:
                seg_for_stt = seg
                if overlap_len > 0:
                    last_tail = seg[-overlap_len:].copy()
            
            # ì „ì‚¬ ì‹¤í–‰
            try:
                # WhisperëŠ” float32 [-1, 1] ë²”ìœ„ í•„ìš”
                seg_for_stt = seg_for_stt.clip(-1.0, 1.0)
                
                kwargs = {"fp16": False}  # CPU ì‚¬ìš©
                if LANG != "auto":
                    kwargs["language"] = LANG
                
                result = model.transcribe(seg_for_stt, **kwargs)
                text = result.get("text", "").strip()
                
                if text:
                    # ê²°ê³¼ë¥¼ íì— ë„£ê¸° (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ í‘œì‹œ)
                    st.session_state.transcript_q.put(text)
                    
            except Exception as e:
                st.session_state.transcript_q.put(f"[Error: {str(e)}]")

# ---------- WebRTC ì„¤ì • ----------
st.info("ğŸ¤ ì•„ë˜ 'START' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•˜ì„¸ìš”.")

rtc_config = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

webrtc_ctx = webrtc_streamer(
    key="whisper-streaming",
    mode=WebRtcMode.SENDRECV,
    rtc_configuration=rtc_config,
    media_stream_constraints={
        "audio": {
            "sampleRate": 48000,
            "channelCount": 1,
            "echoCancellation": True,
            "noiseSuppression": True,
        },
        "video": False
    },
    audio_frame_callback=audio_frame_callback,
    async_processing=True,
)

# ---------- ì»¨íŠ¸ë¡¤ ë²„íŠ¼ ----------
col1, col2, col3 = st.columns(3)

with col1:
    if st.button("ğŸ™ï¸ Start Transcription"):
        if not st.session_state.running:
            st.session_state.running = True
            st.session_state.caption = ""
            
            # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
            if st.session_state.worker_thread is None or not st.session_state.worker_thread.is_alive():
                st.session_state.worker_thread = threading.Thread(
                    target=transcribe_worker, 
                    daemon=True
                )
                st.session_state.worker_thread.start()
            
            st.success("âœ… Transcription started!")

with col2:
    if st.button("â¹ï¸ Stop"):
        st.session_state.running = False
        st.info("â¸ï¸ Transcription stopped.")

with col3:
    if st.button("ğŸ—‘ï¸ Clear"):
        st.session_state.caption = ""
        # í ë¹„ìš°ê¸°
        while not st.session_state.transcript_q.empty():
            try:
                st.session_state.transcript_q.get_nowait()
            except queue.Empty:
                break

# ---------- ìë§‰ í‘œì‹œ (ë©”ì¸ ìŠ¤ë ˆë“œ) ----------
# ì „ì‚¬ íì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
try:
    while not st.session_state.transcript_q.empty():
        text = st.session_state.transcript_q.get_nowait()
        st.session_state.caption += " " + text
        
        # ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ì•ë¶€ë¶„ ì œê±°
        words = st.session_state.caption.split()
        if len(words) > 500:
            st.session_state.caption = " ".join(words[-500:])
except queue.Empty:
    pass

# ìë§‰ í‘œì‹œ
st.markdown("---")
st.subheader("ğŸ“ Live Captions")
caption_container = st.container()
with caption_container:
    if st.session_state.caption:
        st.markdown(f"**{st.session_state.caption}**")
    else:
        st.info("ìŒì„± ì¸ì‹ ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤...")

# ---------- ìƒíƒœ í‘œì‹œ ----------
if webrtc_ctx.state.playing:
    st.success("ğŸ”´ Recording...")
else:
    st.warning("âšª Not recording")

# ìë™ ìƒˆë¡œê³ ì¹¨ (ì „ì‚¬ ê²°ê³¼ ì—…ë°ì´íŠ¸ìš©)
if st.session_state.running:
    time.sleep(0.5)
    st.rerun()

# ---------- ë””ë²„ê·¸ ì •ë³´ ----------
with st.expander("ğŸ”§ Debug Info"):
    st.write(f"Audio queue size: {st.session_state.audio_q.qsize()}")
    st.write(f"Transcript queue size: {st.session_state.transcript_q.qsize()}")
    st.write(f"Worker thread alive: {st.session_state.worker_thread.is_alive() if st.session_state.worker_thread else False}")
